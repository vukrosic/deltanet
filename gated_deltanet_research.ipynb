{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gated DeltaNet Research Notebook\n",
                "\n",
                "This notebook is for researching Gated DeltaNet on Google Colab.\n",
                "\n",
                "**Paper**: [Gated Delta Networks: Improving Mamba2 with Delta Rule](https://arxiv.org/abs/2412.06464)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Check GPU Availability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CUDA available: True\n",
                        "GPU: Tesla T4\n",
                        "GPU Memory: 15.83 GB\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "else:\n",
                "    print(\"⚠️ WARNING: No GPU detected! Go to Runtime > Change runtime type > GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup and Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cloning into 'deltanet'...\n",
                        "remote: Enumerating objects: 16235, done.\u001b[K\n",
                        "remote: Counting objects: 100% (16235/16235), done.\u001b[K\n",
                        "remote: Compressing objects: 100% (4519/4519), done.\u001b[K\n",
                        "remote: Total 16235 (delta 11634), reused 16235 (delta 11634), pack-reused 0 (from 0)\u001b[K\n",
                        "Receiving objects: 100% (16235/16235), 5.88 MiB | 12.53 MiB/s, done.\n",
                        "Resolving deltas: 100% (11634/11634), done.\n",
                        "/content/flash-linear-attention/deltanet\n"
                    ]
                }
            ],
            "source": [
                "# Clone the repository\n",
                "!git clone https://github.com/vukrosic/deltanet.git\n",
                "%cd deltanet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Obtaining file:///content/flash-linear-attention/deltanet\n",
                        "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
                        "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
                        "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
                        "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
                        "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-linear-attention==0.4.1) (2.9.0+cu126)\n",
                        "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from flash-linear-attention==0.4.1) (4.57.2)\n",
                        "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-linear-attention==0.4.1) (0.8.1)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (3.20.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (4.15.0)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (75.2.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (1.14.0)\n",
                        "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (3.6)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (3.1.6)\n",
                        "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (2025.3.0)\n",
                        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (12.6.77)\n",
                        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (12.6.77)\n",
                        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (12.6.80)\n",
                        "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (9.10.2.21)\n",
                        "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (12.6.4.1)\n",
                        "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (11.3.0.4)\n",
                        "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (10.3.7.77)\n",
                        "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (11.7.1.2)\n",
                        "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (12.5.4.2)\n",
                        "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (0.7.1)\n",
                        "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (2.27.5)\n",
                        "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (3.3.20)\n",
                        "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (12.6.77)\n",
                        "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (12.6.85)\n",
                        "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (1.11.1.6)\n",
                        "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-linear-attention==0.4.1) (3.5.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention==0.4.1) (0.36.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention==0.4.1) (2.0.2)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention==0.4.1) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention==0.4.1) (6.0.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention==0.4.1) (2025.11.3)\n",
                        "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention==0.4.1) (2.32.4)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention==0.4.1) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention==0.4.1) (0.7.0)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention==0.4.1) (4.67.1)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->flash-linear-attention==0.4.1) (1.2.0)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-linear-attention==0.4.1) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-linear-attention==0.4.1) (3.0.3)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->flash-linear-attention==0.4.1) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->flash-linear-attention==0.4.1) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->flash-linear-attention==0.4.1) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->flash-linear-attention==0.4.1) (2025.11.12)\n",
                        "Building wheels for collected packages: flash-linear-attention\n",
                        "  Building editable for flash-linear-attention (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
                        "  Created wheel for flash-linear-attention: filename=flash_linear_attention-0.4.1-0.editable-py3-none-any.whl size=6427 sha256=c23ff8267c46dcc33837fe57ac4ee6e2acf5529926984e7071b590c5f5217b04\n",
                        "  Stored in directory: /tmp/pip-ephem-wheel-cache-b6a_t059/wheels/41/d0/36/a81a7c4cb1b511f113149b9c4b029e59f6cb849b5c86664b3a\n",
                        "Successfully built flash-linear-attention\n",
                        "Installing collected packages: flash-linear-attention\n",
                        "  Attempting uninstall: flash-linear-attention\n",
                        "    Found existing installation: flash-linear-attention 0.4.1\n",
                        "    Uninstalling flash-linear-attention-0.4.1:\n",
                        "      Successfully uninstalled flash-linear-attention-0.4.1\n",
                        "Successfully installed flash-linear-attention-0.4.1\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.colab-display-data+json": {
                            "id": "9e7c8011a5394d2ca3677920c0469514",
                            "pip_warning": {
                                "packages": [
                                    "fla"
                                ]
                            }
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
                        "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
                        "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
                    ]
                }
            ],
            "source": [
                "# Install dependencies\n",
                "!pip install -e .\n",
                "!pip install transformers einops"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Import Gated DeltaNet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n",
                        "Imports successful!\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from fla.layers import GatedDeltaNet\n",
                "from fla.models import GatedDeltaNetConfig, GatedDeltaNetForCausalLM, GatedDeltaNetModel\n",
                "\n",
                "# Set device\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Using device: {device}\")\n",
                "print(\"Imports successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Basic Layer Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 128, 512])\n",
                        "Output shape: torch.Size([2, 128, 512])\n",
                        "Input device: cuda:0\n",
                        "Output device: cuda:0\n"
                    ]
                }
            ],
            "source": [
                "# Create a Gated DeltaNet layer and move to GPU\n",
                "layer = GatedDeltaNet(\n",
                "    hidden_size=512,\n",
                "    expand_v=2.0,\n",
                "    head_dim=64,\n",
                "    num_heads=6,  # 6 * 64 = 384 = 0.75 * 512\n",
                "    mode='chunk',\n",
                "    use_gate=True,\n",
                "    use_short_conv=True,\n",
                ").to(device)\n",
                "\n",
                "# Test with random input on GPU\n",
                "batch_size = 2\n",
                "seq_len = 128\n",
                "hidden_size = 512\n",
                "\n",
                "x = torch.randn(batch_size, seq_len, hidden_size, device=device)\n",
                "output, _, _ = layer(x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Input device: {x.device}\")\n",
                "print(f\"Output device: {output.device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model created with 190.83M parameters\n",
                        "Model device: cuda:0\n"
                    ]
                }
            ],
            "source": [
                "# Create a small Gated DeltaNet model\n",
                "config = GatedDeltaNetConfig(\n",
                "    hidden_size=768,\n",
                "    num_hidden_layers=12,\n",
                "    num_heads=12,\n",
                "    head_dim=64,\n",
                "    vocab_size=50257,\n",
                ")\n",
                "\n",
                "model = GatedDeltaNetForCausalLM(config).to(device)\n",
                "print(f\"Model created with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
                "print(f\"Model device: {next(model.parameters()).device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Forward Pass Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test forward pass\n",
                "input_ids = torch.randint(0, config.vocab_size, (2, 64), device=device)\n",
                "outputs = model(input_ids)\n",
                "\n",
                "print(f\"Input IDs shape: {input_ids.shape}\")\n",
                "print(f\"Logits shape: {outputs.logits.shape}\")\n",
                "print(f\"Logits device: {outputs.logits.device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Memory Usage Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
                "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
                "    print(f\"Max GPU memory allocated: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Test Different Sequence Lengths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "# Test different sequence lengths\n",
                "seq_lengths = [64, 128, 256, 512, 1024]\n",
                "batch_size = 2\n",
                "hidden_size = 512\n",
                "\n",
                "layer = GatedDeltaNet(\n",
                "    hidden_size=hidden_size,\n",
                "    expand_v=2.0,\n",
                "    head_dim=64,\n",
                "    num_heads=6,\n",
                "    mode='chunk',\n",
                "    use_gate=True,\n",
                "    use_short_conv=True,\n",
                ").to(device)\n",
                "\n",
                "print(\"Sequence Length | Time (ms) | Memory (MB)\")\n",
                "print(\"-\" * 45)\n",
                "\n",
                "for seq_len in seq_lengths:\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.reset_peak_memory_stats()\n",
                "    \n",
                "    x = torch.randn(batch_size, seq_len, hidden_size, device=device)\n",
                "    \n",
                "    # Warmup\n",
                "    _ = layer(x)\n",
                "    \n",
                "    # Measure time\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.synchronize()\n",
                "    start = time.time()\n",
                "    output, _, _ = layer(x)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.synchronize()\n",
                "    elapsed = (time.time() - start) * 1000\n",
                "    \n",
                "    mem_mb = torch.cuda.max_memory_allocated(0) / 1e6 if torch.cuda.is_available() else 0\n",
                "    \n",
                "    print(f\"{seq_len:14d} | {elapsed:9.2f} | {mem_mb:11.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Research Experiments\n",
                "\n",
                "Add your research experiments below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your experiments here\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Key Architecture Components\n",
                "\n",
                "### Gated DeltaNet Layer Parameters:\n",
                "\n",
                "- **hidden_size**: Hidden dimension\n",
                "- **expand_v**: Value dimension expansion ratio (default: 2.0)\n",
                "- **head_dim**: Dimension per head\n",
                "- **num_heads**: Number of attention heads (num_heads * head_dim = 0.75 * hidden_size when use_gate=True)\n",
                "- **num_v_heads**: Number of value heads (GVA if > num_heads)\n",
                "- **mode**: Kernel mode ('chunk' for training, 'fused_recurrent' for inference)\n",
                "- **use_beta**: Use beta parameter\n",
                "- **use_gate**: Use output gating (recommended: True)\n",
                "- **use_short_conv**: Use short convolutions (crucial for performance!)\n",
                "- **allow_neg_eigval**: Allow negative eigenvalues\n",
                "- **conv_size**: Convolution kernel size (default: 4)\n",
                "\n",
                "### Key Operations:\n",
                "- `chunk_gated_delta_rule`: Chunk-based implementation (training)\n",
                "- `fused_recurrent_gated_delta_rule`: Fused recurrent (inference)\n",
                "\n",
                "### GPU Requirements:\n",
                "- T4 (16GB): Good for models up to ~350M parameters\n",
                "- V100 (16GB): Good for models up to ~1B parameters\n",
                "- A100 (40GB): Good for models up to ~7B parameters"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
